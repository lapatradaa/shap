{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "Dental X-Rays Caries Segmentation (UNet)",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lapatradaa/shap/blob/main/Dental_X_Rays_Caries_Segmentation_(UNet).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'panoramic-dental-dataset:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F3419046%2F6067948%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240731%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240731T090546Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Da9dc3acb75eb353ff4260c61409c6d12d1a60c14d91fcfe54a3d901424a3dac4ea4cad47f1778839024052e98a64e211d20541d293011b5d6b373613a6a966b1b2bb70ae9bb09b02f13bdebfcdb6d0010ebc0aa0c6e11c13c44f8293a93f51c32a8cb9eab5da0e449bde2079923a1e4cea52a2f4bd12c2b0b7a3fff7b82303c070a7a900e96017c0c54a8e3b49bdab6b77391cc5b1017b946b924c0eab3fbe3aa415407263e1d73eaaeea8f0b4923dcd11d388cdfd4880d42f29f4cf58ee394c4a2149565fa18fae718c649378c5d0542fcb73af8f1e9e3e97b5a487b2217115a5311b0054d07862fef762c9df905cfdf7ceff55018286d961281eebdc83eaf5'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "N7Hcl6JoCdiT",
        "outputId": "63a886e6-cc18-491b-a3a1-1855283b91a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading panoramic-dental-dataset, 255581868 bytes compressed\n",
            "[==================================================] 255581868 bytes downloaded\n",
            "Downloaded and uncompressed: panoramic-dental-dataset\n",
            "Data source import complete.\n"
          ]
        }
      ],
      "execution_count": 2
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Description\n",
        "\n",
        "This notebook is baseline for caries segmentation on dental x-ray images for Panoramic Dental Dataset.\n",
        "\n",
        "For segmentation were used Unet + efficientnet_b0 architecture from segmentation_models_pytorch library.\n",
        "\n",
        "`If this notebook was helpful to you, please upvoite. Thank You!`"
      ],
      "metadata": {
        "id": "lhR20d-bCdiV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip -q install segmentation_models_pytorch"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T18:02:11.104252Z",
          "iopub.execute_input": "2023-07-02T18:02:11.104864Z",
          "iopub.status.idle": "2023-07-02T18:02:22.017125Z",
          "shell.execute_reply.started": "2023-07-02T18:02:11.104829Z",
          "shell.execute_reply": "2023-07-02T18:02:22.01594Z"
        },
        "trusted": true,
        "id": "NjVLKeOwCdiW",
        "outputId": "6dcf6732-22e4-44d8-8d95-913d7942ed2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/58.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.5/68.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import warnings\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "\n",
        "warnings.filterwarnings(action='ignore', category=UserWarning)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T18:02:22.020311Z",
          "iopub.execute_input": "2023-07-02T18:02:22.020968Z",
          "iopub.status.idle": "2023-07-02T18:02:25.966019Z",
          "shell.execute_reply.started": "2023-07-02T18:02:22.020927Z",
          "shell.execute_reply": "2023-07-02T18:02:25.965028Z"
        },
        "trusted": true,
        "id": "HZxFlqzYCdiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_everything(seed=42):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "seed_everything()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T18:02:25.967649Z",
          "iopub.execute_input": "2023-07-02T18:02:25.968009Z",
          "iopub.status.idle": "2023-07-02T18:02:25.977126Z",
          "shell.execute_reply.started": "2023-07-02T18:02:25.967971Z",
          "shell.execute_reply": "2023-07-02T18:02:25.975941Z"
        },
        "trusted": true,
        "id": "Y8NoMS2qCdiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T18:02:25.980242Z",
          "iopub.execute_input": "2023-07-02T18:02:25.980777Z",
          "iopub.status.idle": "2023-07-02T18:02:26.021025Z",
          "shell.execute_reply.started": "2023-07-02T18:02:25.980742Z",
          "shell.execute_reply": "2023-07-02T18:02:26.020131Z"
        },
        "trusted": true,
        "id": "oIR_S7sfCdiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create CariesDataset"
      ],
      "metadata": {
        "id": "BqJyQYRkCdiX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CariesDataset(Dataset):\n",
        "    def __init__(self, images_path_list, labels_path_list, augmentation = True, device = 'cpu', image_size = (384, 768)):\n",
        "        self.images = images_path_list\n",
        "        self.labels = labels_path_list\n",
        "        self.augmentation = augmentation\n",
        "        self.device = device\n",
        "\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize(image_size),\n",
        "            transforms.Grayscale(),\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "\n",
        "        if self.augmentation:\n",
        "            self.same_augmentation = transforms.Compose([\n",
        "                transforms.RandomRotation(degrees = 5),\n",
        "                transforms.RandomHorizontalFlip(p = 0.5)\n",
        "            ])\n",
        "\n",
        "            self.different_augmentation = transforms.Compose([\n",
        "                transforms.RandomAdjustSharpness(2),\n",
        "                transforms.ColorJitter(brightness=0.5, contrast=0.5)\n",
        "            ])\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.images[idx])\n",
        "        label = Image.open(self.labels[idx])\n",
        "\n",
        "        if self.augmentation:\n",
        "            seed = np.random.randint(0, 10000)\n",
        "\n",
        "            torch.random.manual_seed(seed)\n",
        "            image = self.same_augmentation(image)\n",
        "            image = self.different_augmentation(image)\n",
        "\n",
        "            torch.random.manual_seed(seed)\n",
        "            label = self.same_augmentation(label)\n",
        "\n",
        "        image = self.transform(image).to(self.device)\n",
        "        label = self.transform(label).to(self.device)\n",
        "\n",
        "        label = 1. * (label != 0)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T18:02:26.024879Z",
          "iopub.execute_input": "2023-07-02T18:02:26.025155Z",
          "iopub.status.idle": "2023-07-02T18:02:26.037726Z",
          "shell.execute_reply.started": "2023-07-02T18:02:26.025131Z",
          "shell.execute_reply": "2023-07-02T18:02:26.037114Z"
        },
        "trusted": true,
        "id": "HyUJrcXJCdiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = '/kaggle/input/panoramic-dental-dataset/images_cut/'\n",
        "labels_path = '/kaggle/input/panoramic-dental-dataset/labels_cut/'\n",
        "\n",
        "file_names = [filename for filename in os.listdir(image_path)]\n",
        "train_files, val_files = train_test_split(file_names, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "train_image_path = [image_path + file_name for file_name in train_files]\n",
        "train_mask_path = [labels_path + file_name for file_name in train_files]\n",
        "\n",
        "eval_image_path = [image_path + file_name for file_name in val_files]\n",
        "eval_mask_path = [labels_path + file_name for file_name in val_files]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T18:02:26.038941Z",
          "iopub.execute_input": "2023-07-02T18:02:26.039523Z",
          "iopub.status.idle": "2023-07-02T18:02:26.058179Z",
          "shell.execute_reply.started": "2023-07-02T18:02:26.039491Z",
          "shell.execute_reply": "2023-07-02T18:02:26.057204Z"
        },
        "trusted": true,
        "id": "p4V-yOqFCdiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = CariesDataset(\n",
        "    images_path_list = train_image_path,\n",
        "    labels_path_list = train_mask_path,\n",
        "    augmentation = True,\n",
        "    device = device,\n",
        ")\n",
        "\n",
        "eval_dataset = CariesDataset(\n",
        "    images_path_list = eval_image_path,\n",
        "    labels_path_list = eval_mask_path,\n",
        "    augmentation = False,\n",
        "    device = device,\n",
        ")\n",
        "\n",
        "toPIL = transforms.ToPILImage()\n",
        "\n",
        "batch_size = 8\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
        "eval_dataloader = DataLoader(eval_dataset, shuffle=False, batch_size=1)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T18:02:26.059765Z",
          "iopub.execute_input": "2023-07-02T18:02:26.060135Z",
          "iopub.status.idle": "2023-07-02T18:02:26.06865Z",
          "shell.execute_reply.started": "2023-07-02T18:02:26.060103Z",
          "shell.execute_reply": "2023-07-02T18:02:26.067913Z"
        },
        "trusted": true,
        "id": "SBsaXwhYCdiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create LossFunction and CalculateMetricFunction"
      ],
      "metadata": {
        "id": "lDqVOHpjCdiX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DiceLoss(nn.Module):\n",
        "    def __init__(self, smooth = 1, activation = None):\n",
        "        super(DiceLoss, self).__init__()\n",
        "        self.smooth = smooth\n",
        "        self.activation = activation\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        if self.activation:\n",
        "            inputs = self.activation(inputs)\n",
        "\n",
        "        inputs = inputs.view(-1)\n",
        "        targets = targets.view(-1)\n",
        "\n",
        "        intersection = (inputs * targets).sum()\n",
        "        dice = (2. * intersection + self.smooth)/(inputs.sum() + targets.sum() + self.smooth)\n",
        "\n",
        "        return 1 - dice\n",
        "\n",
        "\n",
        "\n",
        "def metric_calculate(prediction: np.ndarray, target: np.ndarray):\n",
        "\n",
        "    target = np.uint8(target.flatten() > 0.5)\n",
        "    prediction = np.uint8(prediction.flatten() > 0.5)\n",
        "    TP = (prediction * target).sum()\n",
        "    FN = ((1 - prediction) * target).sum()\n",
        "    TN = ((1 - prediction) * (1 - target)).sum()\n",
        "    FP = (prediction * (1 - target)).sum()\n",
        "\n",
        "    acc = (TP + TN) / (TP + TN + FP + FN + 1e-4)\n",
        "    iou = TP / (TP + FP + FN + 1e-4)\n",
        "    dice = (2 * TP) / (2 * TP + FP + FN + 1e-4)\n",
        "    pre = TP / (TP + FP + 1e-4)\n",
        "    spe = TN / (FP + TN + 1e-4)\n",
        "    sen = TP / (TP + FN + 1e-4)\n",
        "\n",
        "    return acc, iou, dice, pre, spe, sen"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T18:02:26.06983Z",
          "iopub.execute_input": "2023-07-02T18:02:26.070455Z",
          "iopub.status.idle": "2023-07-02T18:02:26.082257Z",
          "shell.execute_reply.started": "2023-07-02T18:02:26.070424Z",
          "shell.execute_reply": "2023-07-02T18:02:26.081605Z"
        },
        "trusted": true,
        "id": "YxHwusFYCdiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create model for semantic segmentation from SMP library"
      ],
      "metadata": {
        "id": "o2FOw2o5CdiX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = smp.UnetPlusPlus(\n",
        "    encoder_name = 'efficientnet-b0',\n",
        "    encoder_weights = 'imagenet',\n",
        "    in_channels = 1,\n",
        "    classes = 1,\n",
        ").to(device)\n",
        "\n",
        "model_name = 'UNetEfficientnetB0'\n",
        "\n",
        "criterion = DiceLoss(activation=F.sigmoid)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "\n",
        "num_epoch = 50"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T18:02:26.083512Z",
          "iopub.execute_input": "2023-07-02T18:02:26.084458Z",
          "iopub.status.idle": "2023-07-02T18:02:27.739573Z",
          "shell.execute_reply.started": "2023-07-02T18:02:26.084426Z",
          "shell.execute_reply": "2023-07-02T18:02:27.738522Z"
        },
        "trusted": true,
        "id": "UmEekMCBCdiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Process"
      ],
      "metadata": {
        "id": "F2Js0re6CdiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print (f'Training {model_name} start.')\n",
        "\n",
        "IoU_max = 0.\n",
        "losses_train, losses_val = [], []\n",
        "metrics = []\n",
        "\n",
        "for epoch in tqdm(range(num_epoch)):\n",
        "    current_train_loss, current_val_loss = 0., 0.\n",
        "    current_metric = np.zeros(6)\n",
        "\n",
        "    model.train()\n",
        "    for images, labels in train_dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(images)\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        current_train_loss += loss.item() / len(train_dataloader)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for images, labels in eval_dataloader:\n",
        "            logits = model(images)\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "            current_val_loss += loss.item() / len(eval_dataloader)\n",
        "            current_metric += np.array(metric_calculate(\n",
        "                logits.cpu().detach().numpy(),\n",
        "                labels.cpu().detach().numpy())) / len(eval_dataloader)\n",
        "\n",
        "    losses_train.append(current_train_loss)\n",
        "    losses_val.append(current_val_loss)\n",
        "    metrics.append(current_metric.tolist())\n",
        "\n",
        "    if IoU_max < metrics[-1][1]:\n",
        "        torch.save(model, f'{model_name}-best.pth')\n",
        "        IoU_max = metrics[-1][1]\n",
        "\n",
        "    print (f'Epoch: {epoch + 1}, train_loss: {losses_train[-1]:.4f}, val_loss: {losses_val[-1]:.4f}, IoU: {metrics[-1][1]:.4f}')\n",
        "\n",
        "\n",
        "log = {}\n",
        "log['train_loss'] = losses_train\n",
        "log['eval_loss'] = losses_val\n",
        "log['metric'] = metrics\n",
        "log['best_score'] = IoU_max\n",
        "\n",
        "torch.save(model, f'{model_name}-last.pth')\n",
        "\n",
        "with open(f'log.txt', 'w') as outfile:\n",
        "    json.dump(log, outfile)\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "print ('- - ' * 30)\n",
        "print (f'Training {model_name} done. Best IoU: {IoU_max:.4f}.')\n",
        "print ('- - ' * 30)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T18:02:46.098429Z",
          "iopub.execute_input": "2023-07-02T18:02:46.098803Z",
          "iopub.status.idle": "2023-07-02T18:14:36.558547Z",
          "shell.execute_reply.started": "2023-07-02T18:02:46.098772Z",
          "shell.execute_reply": "2023-07-02T18:14:36.557416Z"
        },
        "trusted": true,
        "id": "1VUdlM7eCdiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(losses_train, label = 'train_loss')\n",
        "plt.plot(losses_val, label = 'val_loss')\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot([metric[1] for metric in metrics], label = 'IoU')\n",
        "plt.axhline (IoU_max, linestyle = '--', color = 'red', label = f'IoU_max: {IoU_max:.4f}')\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('IoU')\n",
        "\n",
        "plt.savefig('efficientnet-b0.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T18:14:41.286125Z",
          "iopub.execute_input": "2023-07-02T18:14:41.286535Z",
          "iopub.status.idle": "2023-07-02T18:14:41.946197Z",
          "shell.execute_reply.started": "2023-07-02T18:14:41.286503Z",
          "shell.execute_reply": "2023-07-02T18:14:41.945294Z"
        },
        "trusted": true,
        "id": "hOdrzZ0dCdiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference"
      ],
      "metadata": {
        "id": "Fen5dRVKCdiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.load('UNetEfficientnetB0-best.pth').eval()\n",
        "image, label = eval_dataset[6]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T18:30:28.774858Z",
          "iopub.execute_input": "2023-07-02T18:30:28.775441Z",
          "iopub.status.idle": "2023-07-02T18:30:28.975864Z",
          "shell.execute_reply.started": "2023-07-02T18:30:28.775399Z",
          "shell.execute_reply": "2023-07-02T18:30:28.974848Z"
        },
        "trusted": true,
        "id": "8bweV7OECdiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probabilities = F.sigmoid(model(image.unsqueeze(0))).squeeze(0)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T18:30:29.112863Z",
          "iopub.execute_input": "2023-07-02T18:30:29.11365Z",
          "iopub.status.idle": "2023-07-02T18:30:29.138798Z",
          "shell.execute_reply.started": "2023-07-02T18:30:29.113581Z",
          "shell.execute_reply": "2023-07-02T18:30:29.137923Z"
        },
        "trusted": true,
        "id": "BLDpw7DSCdiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (16, 10))\n",
        "\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.imshow(toPIL(image), cmap='gray')\n",
        "plt.title('Input Image')\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.imshow(toPIL(label), cmap='gray')\n",
        "plt.title('GroundTruth Mask')\n",
        "\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.imshow(toPIL(probabilities), cmap='gray')\n",
        "plt.title('Prediction Mask')\n",
        "\n",
        "plt.subplot(2, 2, 4)\n",
        "plt.imshow(toPIL(probabilities), alpha = 0.5, cmap='gray')\n",
        "plt.imshow(toPIL(label), alpha=0.5, cmap='gray')\n",
        "plt.title('Prediction & GroundTruth')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-02T18:30:29.446085Z",
          "iopub.execute_input": "2023-07-02T18:30:29.446489Z",
          "iopub.status.idle": "2023-07-02T18:30:30.461019Z",
          "shell.execute_reply.started": "2023-07-02T18:30:29.446458Z",
          "shell.execute_reply": "2023-07-02T18:30:30.460171Z"
        },
        "trusted": true,
        "id": "xey7GyQoCdiY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}